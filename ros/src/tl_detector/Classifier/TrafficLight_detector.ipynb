{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_detected_traffic_lights(boxes, scores, classes):\n",
    "    inds = ((classes == 10) & (scores >= 0.15)).nonzero()\n",
    "    \n",
    "    filtered_boxes = boxes[inds[0], ...]\n",
    "    filtered_scores = scores[inds[0], ...]\n",
    "    filtered_classes = classes[inds[0], ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "        \n",
    "def crop_detection(image, boxes, path, img):\n",
    "        height = image.shape[1]\n",
    "        width = image.shape[2]\n",
    "        \n",
    "        # Box coordinates are scaled and need to be converted\n",
    "        # into image coordinates\n",
    "        boxes[:, 0] *= height\n",
    "        boxes[:, 1] *= width\n",
    "        boxes[:, 2] *= height\n",
    "        boxes[:, 3] *= width\n",
    "        \n",
    "        # cropping needs integers \n",
    "        boxes = boxes.astype(int)\n",
    "        \n",
    "        # only needed for testing\n",
    "        name = img.split('\\\\')[1]\n",
    "        name = name.split('.')[0]\n",
    "        \n",
    "        # loop over every detected traffic light\n",
    "        for i in range(len(boxes)):\n",
    "            bot, left, top, right = boxes[i,...]\n",
    "\n",
    "            cropped = image[0,bot:top,left:right]\n",
    "            \n",
    "            # only needed for testing\n",
    "            save_img = cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(path+'/'+name+'_'+str(i)+'_cropped.jpg',save_img)\n",
    "            #plt.imshow(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_SSD_MobileNet(path_frozen_model):\n",
    "    #loads a frozen SSD MobileNet\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(path_frozen_model, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "            \n",
    "        # get Tensors for input image + boxes for detected objects + classification for each box\n",
    "        # + scores for how certain the classifier is\n",
    "        image_tensor = graph.get_tensor_by_name('image_tensor:0')\n",
    "        detection_boxes = graph.get_tensor_by_name('detection_boxes:0')\n",
    "        detection_classes = graph.get_tensor_by_name('detection_classes:0')\n",
    "        detection_scores = graph.get_tensor_by_name('detection_scores:0')       \n",
    "    return graph, image_tensor, detection_boxes,detection_scores,detection_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_objects_in_single_img(detection_graph, image_tensor, detection_boxes,\\\n",
    "                                detection_scores, detection_classes, image):\n",
    "  \n",
    "    with tf.Session(graph=detection_graph) as sess:                  \n",
    "        (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                            feed_dict={image_tensor: image})\n",
    "\n",
    "        # Remove unnecessary dimensions\n",
    "        boxes = np.squeeze(boxes)\n",
    "        scores = np.squeeze(scores)\n",
    "        classes = np.squeeze(classes)\n",
    "\n",
    "        return boxes, scores, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frozen_model='frozen_inference_graph.pb'\n",
    "test_images= glob.glob('testing/*.jpg')\n",
    "path_cropped_images = 'testing/cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detection_graph,image_tensor,detection_boxes,detection_scores,detection_classes = load_SSD_MobileNet(frozen_model)\n",
    "for img in test_images[0:2]:\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = np.expand_dims(image, 0)\n",
    "    boxes,scores,classes = detect_objects_in_single_img(detection_graph, image_tensor, detection_boxes,\\\n",
    "                                                        detection_scores, detection_classes, image)\n",
    "    \n",
    "    # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "    boxes, scores, classes = get_detected_traffic_lights(boxes, scores, classes)\n",
    "    crop_detection(image, boxes,path_cropped_images, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cropped_images_red = glob.glob('cropped_sorted_training/red/*.jpg')\n",
    "cropped_images_yellow = glob.glob('cropped_sorted_training/yellow/*.jpg')\n",
    "cropped_images_green = glob.glob('cropped_sorted_training/green/*.jpg')\n",
    "cropped_images_false = glob.glob('cropped_sorted_training/false/*.jpg')\n",
    "\n",
    "labels = []\n",
    "x_data = []\n",
    "\n",
    "for img in cropped_images_red:\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,(64,64))\n",
    "    x_data.append(image)\n",
    "    labels.append(0)\n",
    "    \n",
    "for img in cropped_images_green:\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,(64,64))\n",
    "    x_data.append(image)\n",
    "    labels.append(2)\n",
    "    \n",
    "for img in cropped_images_yellow:\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,(64,64))\n",
    "    x_data.append(image)\n",
    "    labels.append(1)\n",
    "    \n",
    "for img in cropped_images_false:\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,(64,64))\n",
    "    x_data.append(image)\n",
    "    labels.append(4)\n",
    "\n",
    "\n",
    "x_data = np.asarray(x_data)\n",
    "one_hot_labels = to_categorical(labels, num_classes=5)\n",
    "\n",
    "x_train, label_train = shuffle(x_data, one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: x/255 - 0.5, input_shape=(64,64,3)))\n",
    "model.add(Conv2D(6,(5,5),strides=(2, 2), padding='same',activation='relu',\\\n",
    "          use_bias=True,kernel_initializer='TruncatedNormal', bias_initializer='zeros'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='same'))\n",
    "\n",
    "model.add(Conv2D(12,(3,3),strides=(2, 2), padding='same',activation='relu',\\\n",
    "          use_bias=True,kernel_initializer='TruncatedNormal', bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(18,(1,1),strides=(1, 1), padding='valid',activation='relu',\\\n",
    "          use_bias=True,kernel_initializer='TruncatedNormal', bias_initializer='zeros'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100,activation='relu',kernel_initializer='TruncatedNormal',\\\n",
    "                bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10,activation='relu',kernel_initializer='TruncatedNormal',\\\n",
    "                bias_initializer='zeros'))\n",
    "\n",
    "model.add(Dense(5,activation='relu',kernel_initializer='TruncatedNormal',\\\n",
    "                bias_initializer='zeros'))\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
    "\n",
    "model.fit(x=x_train, y=label_train, epochs=5, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
